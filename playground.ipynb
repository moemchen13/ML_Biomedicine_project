{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import ast\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stroke.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model):\n",
    "    if model==\"Logistic Regression\":\n",
    "        ML_model = LogisticRegression()\n",
    "    elif model == \"Decision Tree\":\n",
    "        ML_model = tree.DecisionTreeClassifier()\n",
    "    elif model == \"Random Forest Classificator\":\n",
    "        ML_model = RandomForestClassifier()\n",
    "    elif model == \"Linear Regression\":\n",
    "        ML_model = LinearRegression()\n",
    "    elif model == \"Regression Tree\":\n",
    "        ML_model = tree.DecisionTreeRegressor()\n",
    "    elif model == \"Ridge Regression\":\n",
    "        ML_model = Ridge()\n",
    "    elif model == \"Random Forest Regressor\":\n",
    "        ML_model = RandomForestRegressor()\n",
    "    return ML_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, folder_name='saved_models'):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(folder_name, model_name + '.pkl')\n",
    "\n",
    "    # Save the model using joblib\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"Model saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imputation using random values between min and max for each column\n",
    "def random_impute(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        col = X[:, i]\n",
    "        missing = np.isnan(col)\n",
    "        col_min, col_max = np.nanmin(col), np.nanmax(col)\n",
    "        col[missing] = np.random.uniform(col_min, col_max, size=missing.sum())\n",
    "    return X\n",
    "        \n",
    "def clean_data(X, imputation_strategy='most_frequent', scaling_method='minmax'):\n",
    "    # Separate numerical and non-numerical columns\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Handle missing values based on the chosen imputation strategy\n",
    "    if imputation_strategy in ['mean', 'median','most_frequent']:\n",
    "        imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "        X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    elif imputation_strategy == 'random':        \n",
    "        X_imputed = random_impute(X.copy())  # Apply random imputation before scaling\n",
    "    else:\n",
    "        raise ValueError(\"Invalid imputation strategy. Choose 'mean', 'median', 'most_frequent' or 'random'.\")\n",
    "    \n",
    "    # Encode non-numerical columns with integer encoding\n",
    "    X_imputed[categorical_cols] = X_imputed[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "    # Apply scaling based on the chosen scaling method\n",
    "    if scaling_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaling_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling method. Choose 'minmax' or 'standard'.\")\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    X_cleaned = pd.DataFrame.from_records(data=X_scaled, columns=X.columns)\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_test_split, train_config):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split\n",
    "    param_grid = model[\"params\"]\n",
    "    method = select_model(model[\"model_type\"])\n",
    "    metric = train_config[\"metric\"]\n",
    "    folds = train_config[\"CV\"]\n",
    "    if folds == False:\n",
    "        folds = None \n",
    "        \n",
    "    if \"n_iter\" in model.keys():\n",
    "        # Create a RandomizedSearch object with cross-validation    \n",
    "        searchCV = RandomizedSearchCV(method, param_grid, n_iter=model[\"n_iter\"], cv=folds, n_jobs=-1, scoring=metric, random_state=train_config[\"seed\"], return_train_score=True)\n",
    "    else:\n",
    "        for p in param_grid:\n",
    "            if not isinstance(param_grid[p], list):\n",
    "                param_grid[p] = [param_grid[p]]\n",
    "        searchCV = GridSearchCV(method, param_grid, cv=folds, n_jobs=-1, scoring=metric, return_train_score=True)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    searchCV.fit(X_train, y_train)\n",
    "    \n",
    "    # Test model on test dataset\n",
    "    test_score =  searchCV.score(X_test, y_test)\n",
    "    \n",
    "    return searchCV.best_estimator_, searchCV.best_params_, searchCV.best_score_, searchCV.cv_results_, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Pipeline(data, json_config):    \n",
    "    config = json.loads(json_config)\n",
    "    \n",
    "    models = {**config[\"Runs\"], **config[\"Models\"]}\n",
    "    train_config = config[\"Training\"]\n",
    "    names = list(models.keys())    \n",
    "\n",
    "    target = train_config[\"target\"]\n",
    "    \n",
    "    df = clean_data(data)\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    data_split = train_test_split(X, y, test_size = train_config['train_test_split'], shuffle=True, random_state=train_config['seed'])\n",
    "    \n",
    "    stats = []\n",
    "    hyperparams = []\n",
    "    trained_models = [] \n",
    "\n",
    "    for model in enumerate(models):\n",
    "        trained_model, best_params, val_score, cv_results, test_score= train_model(models[model[1]], data_split, train_config)\n",
    "        \n",
    "        s = {\"validation_score\": val_score, \"cv_summary\": pd.DataFrame(cv_results), \"test_score\": test_score}\n",
    "        stats.append(s)\n",
    "        trained_models.append(trained_model)\n",
    "        hyperparams.append(best_params)\n",
    "    return stats, trained_models, hyperparams, names\n",
    "\n",
    "def get_best_model(stats, trained_models):\n",
    "    index = np.argmin(stats[1,:])\n",
    "    best_value = np.min(stats[1,:])\n",
    "    model= trained_models[index]\n",
    "    return model,best_value,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "10 fits failed out of a total of 25.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.95818011        nan        nan 0.95818011 0.958997  ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the train scores are non-finite: [0.95852732        nan        nan 0.95852732 0.95899698]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'n', 'u', 'm']\n",
      "4\n",
      "Model l:\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.015557      0.004254         0.007362        0.002560   \n",
      "1       0.014515      0.002406         0.008294        0.002638   \n",
      "2       0.010871      0.001725         0.007140        0.004096   \n",
      "3       0.021137      0.017394         0.007457        0.002019   \n",
      "4       0.013639      0.004079         0.005478        0.000959   \n",
      "\n",
      "  param_splitter  param_min_weight_fraction_leaf  param_min_samples_split  \\\n",
      "0           best                             0.0                       15   \n",
      "1           best                             0.2                        2   \n",
      "2         random                             0.2                        2   \n",
      "3           best                             0.0                       15   \n",
      "4           best                             0.0                        2   \n",
      "\n",
      "   param_min_samples_leaf  param_max_leaf_nodes param_max_features  ...  \\\n",
      "0                      10                     2               sqrt  ...   \n",
      "1                      10                   100               log2  ...   \n",
      "2                       1                     2               log2  ...   \n",
      "3                      10                     2               sqrt  ...   \n",
      "4                      10                   100               log2  ...   \n",
      "\n",
      "   mean_test_score std_test_score rank_test_score  split0_train_score  \\\n",
      "0         0.958997       0.000195               1            0.958954   \n",
      "1         0.958997       0.000195               1            0.958954   \n",
      "2         0.958997       0.000195               1            0.958954   \n",
      "3         0.958997       0.000195               1            0.958954   \n",
      "4         0.958997       0.000195               1            0.958954   \n",
      "\n",
      "   split1_train_score  split2_train_score  split3_train_score  \\\n",
      "0            0.959057            0.959057            0.958959   \n",
      "1            0.959057            0.959057            0.958959   \n",
      "2            0.959057            0.959057            0.958959   \n",
      "3            0.959057            0.959057            0.958959   \n",
      "4            0.959057            0.959057            0.958959   \n",
      "\n",
      "   split4_train_score  mean_train_score  std_train_score  \n",
      "0            0.958959          0.958997         0.000049  \n",
      "1            0.958959          0.958997         0.000049  \n",
      "2            0.958959          0.958997         0.000049  \n",
      "3            0.958959          0.958997         0.000049  \n",
      "4            0.958959          0.958997         0.000049  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "Model n:\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       3.460837      0.056285         0.103638        0.007874   \n",
      "1       0.604571      0.018056         0.035695        0.011067   \n",
      "2       0.710125      0.039976         0.025828        0.004071   \n",
      "3       2.894150      0.078830         0.108092        0.020249   \n",
      "4      12.239390      1.976103         0.249472        0.049078   \n",
      "\n",
      "   param_n_estimators  param_min_samples_split  param_min_samples_leaf  \\\n",
      "0                 500                        2                       4   \n",
      "1                 100                        2                       1   \n",
      "2                 100                        2                       4   \n",
      "3                 500                       10                       4   \n",
      "4                 500                        2                       1   \n",
      "\n",
      "  param_max_features  param_max_depth param_criterion  ...  mean_test_score  \\\n",
      "0               log2                1         entropy  ...         0.958997   \n",
      "1               log2                1         entropy  ...         0.958997   \n",
      "2               log2                1        log_loss  ...         0.958997   \n",
      "3               log2                1         entropy  ...         0.958997   \n",
      "4               sqrt               20            gini  ...         0.953524   \n",
      "\n",
      "  std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0       0.000195                1            0.958954            0.959057   \n",
      "1       0.000195                1            0.958954            0.959057   \n",
      "2       0.000195                1            0.958954            0.959057   \n",
      "3       0.000195                1            0.958954            0.959057   \n",
      "4       0.003208                5            1.000000            0.999489   \n",
      "\n",
      "   split2_train_score  split3_train_score  split4_train_score  \\\n",
      "0            0.959057            0.958959            0.958959   \n",
      "1            0.959057            0.958959            0.958959   \n",
      "2            0.959057            0.958959            0.958959   \n",
      "3            0.959057            0.958959            0.958959   \n",
      "4            1.000000            1.000000            1.000000   \n",
      "\n",
      "   mean_train_score  std_train_score  \n",
      "0          0.958997         0.000049  \n",
      "1          0.958997         0.000049  \n",
      "2          0.958997         0.000049  \n",
      "3          0.958997         0.000049  \n",
      "4          0.999898         0.000204  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Model u:\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_solver  \\\n",
      "0       0.103088      0.008954         0.008228        0.003792    newton-cg   \n",
      "1       0.003629      0.000534         0.000000        0.000000          sag   \n",
      "2       0.004133      0.000570         0.000000        0.000000    newton-cg   \n",
      "3       0.080423      0.010591         0.004576        0.000460    newton-cg   \n",
      "4       0.042162      0.006478         0.005176        0.002003    liblinear   \n",
      "\n",
      "  param_penalty  param_max_iter  param_C  \\\n",
      "0            l2            1000   100.00   \n",
      "1    elasticnet            1000   100.00   \n",
      "2            l1             100     0.01   \n",
      "3          None             100   100.00   \n",
      "4            l2             100     0.01   \n",
      "\n",
      "                                              params  split0_test_score  ...  \\\n",
      "0  {'solver': 'newton-cg', 'penalty': 'l2', 'max_...           0.958350  ...   \n",
      "1  {'solver': 'sag', 'penalty': 'elasticnet', 'ma...                NaN  ...   \n",
      "2  {'solver': 'newton-cg', 'penalty': 'l1', 'max_...                NaN  ...   \n",
      "3  {'solver': 'newton-cg', 'penalty': None, 'max_...           0.958350  ...   \n",
      "4  {'solver': 'liblinear', 'penalty': 'l2', 'max_...           0.959167  ...   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
      "0         0.958180        0.000560                2            0.958444   \n",
      "1              NaN             NaN                4                 NaN   \n",
      "2              NaN             NaN                4                 NaN   \n",
      "3         0.958180        0.000560                2            0.958444   \n",
      "4         0.958997        0.000195                1            0.958954   \n",
      "\n",
      "   split1_train_score  split2_train_score  split3_train_score  \\\n",
      "0            0.958138            0.958750            0.958448   \n",
      "1                 NaN                 NaN                 NaN   \n",
      "2                 NaN                 NaN                 NaN   \n",
      "3            0.958138            0.958750            0.958448   \n",
      "4            0.959057            0.959057            0.958959   \n",
      "\n",
      "   split4_train_score  mean_train_score  std_train_score  \n",
      "0            0.958857          0.958527         0.000254  \n",
      "1                 NaN               NaN              NaN  \n",
      "2                 NaN               NaN              NaN  \n",
      "3            0.958857          0.958527         0.000254  \n",
      "4            0.958959          0.958997         0.000049  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Model m:\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_C  \\\n",
      "0       0.083456       0.01018         0.006288        0.001238     0.01   \n",
      "\n",
      "   param_max_iter param_penalty param_solver  \\\n",
      "0             100          None    newton-cg   \n",
      "\n",
      "                                              params  split0_test_score  ...  \\\n",
      "0  {'C': 0.01, 'max_iter': 100, 'penalty': None, ...            0.95835  ...   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
      "0          0.95818         0.00056                1            0.958444   \n",
      "\n",
      "   split1_train_score  split2_train_score  split3_train_score  \\\n",
      "0            0.958138             0.95875            0.958448   \n",
      "\n",
      "   split4_train_score  mean_train_score  std_train_score  \n",
      "0            0.958857          0.958527         0.000254  \n",
      "\n",
      "[1 rows x 24 columns]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stats, trained_models, hyperparams, names = ML_Pipeline(df, '{\"Runs\": {\"l\": {\"model_type\": \"Decision Tree\", \"params\": {\"max_depth\": [1, 20], \"min_samples_split\": [2, 15], \"min_samples_leaf\": [1, 10], \"max_features\": [\"sqrt\", \"log2\"], \"max_leaf_nodes\": [2, 100], \"min_weight_fraction_leaf\": [0.0, 0.2], \"criterion\": [\"gini\", \"entropy\", \"log_loss\"], \"splitter\": [\"best\", \"random\"]}, \"n_iter\": 5}, \"n\": {\"model_type\": \"Random Forest Classificator\", \"params\": {\"n_estimators\": [100, 500], \"max_depth\": [1, 20], \"min_samples_split\": [2, 10], \"min_samples_leaf\": [1, 4], \"max_features\": [\"sqrt\", \"log2\"], \"bootstrap\": [true, false], \"criterion\": [\"gini\", \"entropy\", \"log_loss\"]}, \"n_iter\": 5}, \"u\": {\"model_type\": \"Logistic Regression\", \"params\": {\"penalty\": [\"l1\", \"l2\", \"elasticnet\", null], \"C\": [0.01, 100.0], \"solver\": [\"newton-cg\", \"liblinear\", \"sag\", \"saga\"], \"max_iter\": [100, 1000]}, \"n_iter\": 5}}, \"Models\": {\"m\": {\"model_type\": \"Logistic Regression\", \"params\": {\"penalty\": null, \"C\": 0.01, \"solver\": \"newton-cg\", \"max_iter\": 100}}}, \"Training\": {\"is_regression\": false, \"CV\": false, \"metric\": null, \"target\": \"stroke\", \"seed\": 42, \"train_test_split\": 0.2}}')\n",
    "print(names)\n",
    "print(len(stats))\n",
    "for i,model_stats in enumerate(stats):\n",
    "    print(\"Model \" + names[i] + \":\")\n",
    "    print(model_stats[\"cv_summary\"])\n",
    "            #st.dataframe(model_stats[\"cv_summary\"])\n",
    "            \n",
    "    print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: saved_models\\test_model_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "save_model(trained_models[0], \"test_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='log_loss', max_depth=1, n_estimators=200)\n"
     ]
    }
   ],
   "source": [
    "t = joblib.load(\"saved_models/test_model_1.pkl\")\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
