{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stroke.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model):\n",
    "    if model==\"Logistic Regression\":\n",
    "        ML_model = LogisticRegression()\n",
    "    elif model == \"Decision Tree\":\n",
    "        ML_model = tree.DecisionTreeClassifier()\n",
    "    elif model == \"Random Forest Classificator\":\n",
    "        ML_model = RandomForestClassifier()\n",
    "    elif model == \"Linear Regression\":\n",
    "        ML_model = LinearRegression()\n",
    "    elif model == \"Regression Tree\":\n",
    "        ML_model = tree.DecisionTreeRegressor()\n",
    "    elif model == \"Ridge Regression\":\n",
    "        ML_model = Ridge()\n",
    "    elif model == \"Random Forest Regressor\":\n",
    "        ML_model = RandomForestRegressor()\n",
    "    return ML_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, folder_name='saved_models'):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(folder_name, model_name + '.pkl')\n",
    "\n",
    "    # Save the model using joblib\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"Model saved at: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imputation using random values between min and max for each column\n",
    "def random_impute(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        col = X[:, i]\n",
    "        missing = np.isnan(col)\n",
    "        col_min, col_max = np.nanmin(col), np.nanmax(col)\n",
    "        col[missing] = np.random.uniform(col_min, col_max, size=missing.sum())\n",
    "    return X\n",
    "        \n",
    "def clean_data(X, imputation_strategy='most_frequent', scaling_method='minmax'):\n",
    "    # Separate numerical and non-numerical columns\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Handle missing values based on the chosen imputation strategy\n",
    "    if imputation_strategy in ['mean', 'median','most_frequent']:\n",
    "        imputer = SimpleImputer(strategy=imputation_strategy)\n",
    "        X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    elif imputation_strategy == 'random':        \n",
    "        X_imputed = random_impute(X.copy())  # Apply random imputation before scaling\n",
    "    else:\n",
    "        raise ValueError(\"Invalid imputation strategy. Choose 'mean', 'median', 'most_frequent' or 'random'.\")\n",
    "    \n",
    "    # Encode non-numerical columns with integer encoding\n",
    "    X_imputed[categorical_cols] = X_imputed[categorical_cols].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "    # Apply scaling based on the chosen scaling method\n",
    "    if scaling_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaling_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling method. Choose 'minmax' or 'standard'.\")\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    X_cleaned = pd.DataFrame.from_records(data=X_scaled, columns=X.columns)\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_test_split, train_config):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split\n",
    "    param_grid = model[\"params\"]\n",
    "    method = select_model(model[\"model_type\"])\n",
    "    metric = train_config[\"metric\"]\n",
    "    folds = train_config[\"CV\"]\n",
    "    if folds == False:\n",
    "        folds = None \n",
    "        \n",
    "    if \"n_iter\" in model.keys():\n",
    "        # Create a RandomizedSearch object with cross-validation    \n",
    "        searchCV = RandomizedSearchCV(method, param_grid, n_iter=model[\"n_iter\"], cv=folds, n_jobs=-1, scoring=metric, random_state=train_config[\"seed\"], return_train_score=True)\n",
    "    else:\n",
    "        for p in param_grid:\n",
    "            if not isinstance(param_grid[p], list):\n",
    "                param_grid[p] = [param_grid[p]]\n",
    "        searchCV = GridSearchCV(method, param_grid, cv=folds, n_jobs=-1, scoring=metric, return_train_score=True)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    searchCV.fit(X_train, y_train)\n",
    "    \n",
    "    # Test model on test dataset\n",
    "    test_score =  searchCV.score(X_test, y_test)\n",
    "    \n",
    "    return searchCV.best_estimator_, searchCV.best_params_, searchCV.best_score_, searchCV.cv_results_, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_Pipeline(data, json_config):\n",
    "    with open(json_config) as json_data:\n",
    "        config = json.load(json_data)\n",
    "    \n",
    "    models = {**config[\"Runs\"], **config[\"Models\"]}\n",
    "    train_config = config[\"Training\"]\n",
    "\n",
    "    target = train_config[\"target\"]\n",
    "    \n",
    "    df = clean_data(data)\n",
    "    X = df.drop(target, axis=1)\n",
    "    y = df[target]\n",
    "    \n",
    "    data_split = train_test_split(X, y, test_size = train_config['train_test_split'], shuffle=True, random_state=train_config['seed'])\n",
    "    \n",
    "    stats = []\n",
    "    hyperparams = []\n",
    "    trained_models = []     \n",
    "\n",
    "    for model in enumerate(models):\n",
    "        trained_model, best_params, val_score, cv_results, test_score= train_model(models[model[1]], data_split, train_config)\n",
    "        \n",
    "        s = {\"validation_score\": val_score, \"cv_summary\": pd.DataFrame(cv_results)}\n",
    "        stats.append(s)\n",
    "        trained_models.append(trained_model)\n",
    "        hyperparams.append(best_params)\n",
    "    return stats, trained_models, hyperparams\n",
    "\n",
    "def get_best_model(stats, trained_models):\n",
    "    index = np.argmin(stats[1,:])\n",
    "    best_value = np.min(stats[1,:])\n",
    "    model= trained_models[index]\n",
    "    return model,best_value,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "15 fits failed out of a total of 25.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "13 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.958997          nan 0.95809828        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the train scores are non-finite: [       nan 0.95899698        nan 0.96432654        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "15 fits failed out of a total of 25.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.95883357        nan 0.95711818        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the train scores are non-finite: [       nan 0.9607735         nan 0.97288242        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jonathan\\miniconda3\\envs\\streamlit_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1197: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stats, trained_models, hyperparams = ML_Pipeline(df, \"configuration.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: saved_models\\test_model_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "save_model(trained_models[0], \"test_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='log_loss', max_depth=1, n_estimators=200)\n"
     ]
    }
   ],
   "source": [
    "t = joblib.load(\"saved_models/test_model_1.pkl\")\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
